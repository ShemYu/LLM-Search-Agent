{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                     0.0.274\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'adore programmer.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat([HumanMessage(content=\"Translate this sentence from English to French: I love programming.\")])\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'messages', 'AIMessage'],\n",
       " 'kwargs': {'content': \"J'adore programmer.\", 'additional_kwargs': {}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "Memory allows the AI to remember the context of human interactions. This memory is preserved by creating summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"Hi! Nice to meet you, I'm Shem and I am a programer!\")\n",
    "memory.chat_memory.add_ai_message(\"Oh! Nice to meet you too, I'm Audrey and I also a programer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi! Nice to meet you, I'm Shem and I am a programer!\\nAI: Oh! Nice to meet you too, I'm Audrey and I also a programer!\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # when you use Buffer Memory, memory will completly saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi! Nice to meet you, I'm Jack and I am a house keeper!\\nAI: Oh! Nice to meet you too, I'm Black and I also a house keeper!\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.chat_memory.add_user_message(\"Hi! Nice to meet you, I'm Shem and I am a programer!\")\n",
    "memory.chat_memory.add_ai_message(\"Oh! Nice to meet you too, I'm Audrey and I also a programer!\")\n",
    "memory.chat_memory.add_user_message(\"Hi! Nice to meet you, I'm Jack and I am a house keeper!\")\n",
    "memory.chat_memory.add_ai_message(\"Oh! Nice to meet you too, I'm Black and I also a house keeper!\")\n",
    "memory.load_memory_variables({}) # When you use a window buffer, the memory is saved based on the 'k' steps you've set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n",
    "memory.save_context(inputs={\"User\":\"Hi! Nice to meet you, I'm Shem and I am a programer!\"}, outputs={\"AI\":\"Oh! Nice to meet you too, I'm Audrey and I also a programer!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [SystemMessage(content='\\nThe human introduces themselves as Shem, a programmer, and the AI introduces themselves as Audrey, also a programmer.', additional_kwargs={})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impletment chat with memory\n",
    "\n",
    "Using the SummaryMemory we just created, we can ask the AI for the name we added to the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "System: \n",
      "The human introduces themselves as Shem, a programmer, and the AI introduces themselves as Audrey, also a programmer.\n",
      "Human: Do you remember whats my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Do you remember whats my name?',\n",
       " 'chat_history': [SystemMessage(content='\\nThe human introduces themselves as Shem, a programmer, and the AI introduces themselves as Audrey, also a programmer.', additional_kwargs={})],\n",
       " 'text': \"Yes, your name is Shem. It's nice to meet you, Shem! How can I assist you today?\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Prompt \n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "conversation({\"question\": \"Do you remember whats my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Arguments Generative, RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0 0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n",
      "100 10.1M  100 10.1M    0     0  1474k      0  0:00:07  0:00:07 --:--:-- 10.8M\n"
     ]
    }
   ],
   "source": [
    "! curl -L -o ../data/TSMC2023Q1.zip \"https://drive.google.com/uc?export=download&id=1Z_ww5ZASdIq0uZrg8jt_d7E_PzFQdYH-\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  TSMC2023Q1.zip\n",
      "  inflating: TSMC2023Q1/20230330 TSMC Citi.pdf  \n",
      "  inflating: TSMC2023Q1/20230331 TSMC UBS.pdf  \n",
      "  inflating: TSMC2023Q1/20230406 TSMC MS.pdf  \n",
      "  inflating: TSMC2023Q1/20230410 TSMC DW.pdf  \n",
      "  inflating: TSMC2023Q1/20230410 TSMC HTI.pdf  \n",
      "  inflating: TSMC2023Q1/20230410 TSMC JPM.pdf  \n",
      "  inflating: TSMC2023Q1/20230411 TSMC GS.pdf  \n",
      "  inflating: TSMC2023Q1/20230412 TSMC CL.pdf  \n",
      "  inflating: TSMC2023Q1/20230414 TSMC NMR.pdf  \n",
      "  inflating: TSMC2023Q1/20230417 TSMC HSBC.pdf  \n"
     ]
    }
   ],
   "source": [
    "! cd ../data && unzip TSMC2023Q1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pypdf                         3.15.4\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('../data/TSMC2023Q1/20230330 TSMC Citi.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = loader.load()\n",
    "len(pages) # pages of the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See Appendix A-1 for Analyst Certification, Important Disclosures and Research Analyst Affiliations.\n",
      "Citi Research is a division of Citigroup Global Markets Inc. (the \"Firm\"), which does and seeks to do business with companies covered in its research \n",
      "reports. As a result, investors should be aware that the Firm may have a conflict of interest that could affect the objectivity of this report. Investors should \n",
      "consider this report as only a single factor in making their investment decision. Certain products (not inconsistent with the author's published research) are \n",
      "available only on Citi's portals.30 Mar 2023 07:47:50 ET │ 15 pages    TSMC (2330.TW)Assessing the Potential Upside from AI CITI'S TAKE AI is witnessing a potential “iPhone moment” and should be a L-T catalyst in \n",
      "the semi industry, despite still small contribution at current early stage. nVidia \n",
      "dominates the data center GPU market for now, while we expect to see more \n",
      "AI chips in future designed by hyperscalers, including Google’s TPU, AWS’ \n",
      "ASIC, and the upcoming ASIC from Microsoft that adopts TSMC’s 6nm \n",
      "process. While we believe nVidia only accounts for single-digit % of TSMC’s \n",
      "sales, we project it to contribute >10% of TSMC’s revenue in three years.GPU-based data center is at early stage — Thanks to emergence of generative AI \n",
      "applications, various media (incl. DigiTimes 23 Feb) reported rush orders to TSMC \n",
      "from nVidia. Based on IDC’s report, c.3m units of data center GPU shipped in 2022, \n",
      "which indicated an AI server penetration rate of only 5% or less, if we assume typically \n",
      "4-8 GPU cards installed in an AI server. On expectation of AI data center penetration \n",
      "rising to 20% or higher, there could be potential demand of 30m or more units of data \n",
      "center GPU, which would imply estimated c.10% TSMC revenue upside in 2025E.Emerging opportunity for ASIC — Traditional computer and server architectures \n",
      "are often not efficient for AI tasks, which require large amounts of data processing \n",
      "and complex calculations. For AI inference, which involves using a trained model to \n",
      "make predictions or classifications on new data, the hardware architecture may \n",
      "include GPU, TPU (Tensor processing units), and FPGAs. GPUs are particularly \n",
      "popular for AI inference because they can perform many parallel calculations \n",
      "simultaneously, making them ideal for processing large datasets. For AI training, \n",
      "which involves updating a model based on new data, the hardware may require more \n",
      "powerful processors such as CPUs and specialized accelerators. In addition to the \n",
      "specialized processors, HBM and interconnects such as Compute Express Link are \n",
      "also critical for training workloads. We are seeing all of the aforementioned chips \n",
      "require advanced process nodes, which are mostly offered by TSMC. Near-term smartphone headwinds to continue, yet AI / data center should \n",
      "provide longer-term upside — As we still don’t see smartphone SoC makers’ \n",
      "restocking in foundry, we expect TSMC’s 2Q23E sales down 8% QoQ, followed by a \n",
      "12% QoQ recovery in 3Q23E, on the back of its N3 ramp-up for new iPhone Pro \n",
      "series. Despite lackluster demand in consumer market, we expect TSMC to be able \n",
      "to outperform peers and maintain YoY flat or slightly declining revenue in 2023E. We \n",
      "anticipate a better 2024E outlook thanks to its solid pipeline in AI and HPC. Reiterate \n",
      "Buy with unchanged target price of NT$620. Laura (Chia Yi) Chen AC+886-2-8726-9090laura.cy.chen@citi.comJack Chen+886-2-8726-9091jack1.chen@citi.com Buy Price (30 Mar 23 13:30) NT$535.00 Target price NT$620.00 Expected share price \n",
      "return15.9%Expected dividend yield 2.2% Expected total return 18.1% Market CapNT$13,873,882MUS$454,852MPrice Performance\n",
      "(RIC: 2330.TW, BB: 2330 TT)\n",
      "Earnings Summary              Year toNet ProfitDiluted EPSEPS growth P/EP/BROEYield 31 Dec (NT$M)(NT$) (%)(x)(x)(%)(%) 2021A 596,54023.0015.223.36.429.72.1 2022A1,016,53039.2070.413.64.739.62.1 2023E 872,09633.63-14.215.93.926.92.2 2024E1,019,38339.3116.913.63.326.32.3 2025E1,276,39749.2225.210.92.727.22.4Source: Powered by dataCentral      Citi Research\n",
      " A C T I O NPrepared for Maggie Lee\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content) # content of page 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting posts into chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/shemyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "splitter = NLTKTextSplitter(chunk_size = 1000, chunk_overlap = 200) # 1000 per chunk and overlap with 20%\n",
    "chunks = splitter.split_text(pages[0].page_content) # split page 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks) # page 1 is split into 5 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb                      0.4.7\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma # Langchian imtegrated Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings # Encoder\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "persist_directory = './test_chroma'\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=chunks, # the Chunks input\n",
    "    embedding = embedding, # Embedding Engine\n",
    "    persist_directory=persist_directory # Storage placed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval by AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "result = qa_chain({\"query\": '#zh-tw 請問citi 對台積電的看法? 請用正體中文'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '#zh-tw 請問citi 對台積電的看法? 請用正體中文',\n",
       " 'result': 'Citi對台積電的看法是積極的。他們預期台積電在人工智慧和高性能運算方面的項目將帶來更好的2024年展望。他們重申買入評級，目標價維持在NT$620。他們認為台積電在人工智慧領域有著巨大的潛力，儘管目前貢獻仍然較小。'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Prompt \n",
    "message_template = \"\"\"你是一名專業投資理財顧問，\n",
    "請依據財報內容給予專業的理財建議。\n",
    "給出的財報內容分為三大段落：\n",
    "1. 你的文字建議，文字字數請控制在40字內\n",
    "2. 第二部分重點財報指標，請整理成視覺化的表格\n",
    "\n",
    "現在請用正體中文回應我的問題：{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=message_template,\n",
    "    input_variables=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain_type_kwargs = {\"prompt\": prompt}\n",
    "retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    # chain_type_kwargs=chain_type_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = message_template.format(question=\"請問citi 對台積電的看法? 請用正體中文\")\n",
    "result = retriever({\"query\": query_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 建議：Citi對台積電的看法樂觀，預期其2024年表現將因AI和HPC的穩定產品線而改善，並重申買入評級，目標價格為NT$620。\n",
      "\n",
      "2. 財報指標視覺化表格：\n",
      "\n",
      "| 年份 | 淨利潤 (NT$M) | 每股盈餘 (NT$) | EPS成長 (%) | P/E | P/B | ROE (%) | 殖利率 (%) |\n",
      "|------|--------------|--------------|-------------|-----|-----|---------|------------|\n",
      "| 2021A | 596,540 | 23.00 | 15.2 | 23.3 | 6.4 | 29.7 | 2.1 |\n",
      "| 2022A | 1,016,530 | 39.20 | 70.4 | 13.6 | 4.7 | 39.6 | 2.1 |\n",
      "| 2023E | 872,096 | 33.63 | -14.2 | 15.9 | 3.9 | 26.9 | 2.2 |\n",
      "| 2024E | 1,019,383 | 39.31 | 16.9 | 13.6 | 3.3 | 26.3 | 2.3 |\n",
      "| 2025E | 1,276,397 | 49.22 | 25.2 | 10.9 | 2.7 | 27.2 | 2.4 |\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 年份 | 淨利潤 (NT$M) | 每股盈餘 (NT$) | EPS成長 (%) | P/E | P/B | ROE (%) | 殖利率 (%) |\n",
    "|------|--------------|--------------|-------------|-----|-----|---------|------------|\n",
    "| 2021A | 596,540 | 23.00 | 15.2 | 23.3 | 6.4 | 29.7 | 2.1 |\n",
    "| 2022A | 1,016,530 | 39.20 | 70.4 | 13.6 | 4.7 | 39.6 | 2.1 |\n",
    "| 2023E | 872,096 | 33.63 | -14.2 | 15.9 | 3.9 | 26.9 | 2.2 |\n",
    "| 2024E | 1,019,383 | 39.31 | 16.9 | 13.6 | 3.3 | 26.3 | 2.3 |\n",
    "| 2025E | 1,276,397 | 49.22 | 25.2 | 10.9 | 2.7 | 27.2 | 2.4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
